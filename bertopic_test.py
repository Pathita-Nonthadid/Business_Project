# -*- coding: utf-8 -*-
"""BERTopic_Test.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ymY5ZaZkAs_mkJtsLtdX0SoYLSnPXHDJ
"""

# Install all libraries for BERTopic
!pip install bertopic
!pip install umap-learn hdbscan sentence-transformers

# Upload cleaned_data.csv file from computer
from google.colab import files
uploaded = files.upload()

# Import data into pandas
import pandas as pd
df = pd.read_csv('cleaned_data.csv')

from sklearn.feature_extraction.text import CountVectorizer
from bertopic import BERTopic
from umap import UMAP
from hdbscan import HDBSCAN
from sentence_transformers import SentenceTransformer

# Set all models
embedding_model = SentenceTransformer("all-MiniLM-L6-v2")
umap_model = UMAP(n_neighbors=15, n_components=5, min_dist=0.0, metric='cosine')
hdbscan_model = HDBSCAN(min_cluster_size=15, metric='euclidean', cluster_selection_method='eom', prediction_data=True)

# Custom vectorizer
vectorizer_model = CountVectorizer(min_df=1) # I will change min_df(ignoring words that appear less than n times) in real dataset because it's error in this 500 rows test dataset.
                                             # This error is happening because the CountVectorizer inside topic_model still has min_df set to a value higher than what works for your data.
# BERTopic model with everything
topic_model = BERTopic(
    embedding_model=embedding_model,
    umap_model=umap_model,
    hdbscan_model=hdbscan_model,
    vectorizer_model=vectorizer_model,
    calculate_probabilities=True,
    verbose=True
)

docs = df['text_lemmatized'].tolist()
# Filter out any non-string values from the docs list
docs = [str(doc) for doc in docs if isinstance(doc, str)]

# Print the number of documents and a sample
print(f"Number of documents after filtering: {len(docs)}")
if len(docs) > 0:
    print("Sample documents:")
    for i, doc in enumerate(docs[:5]): # Print the first 5 documents
        print(f"{i+1}: {doc}")
else:
    print("No documents remaining after filtering.")

topics, probs = topic_model.fit_transform(docs)

freq = topic_model.get_topic_info(); freq.head(5)

topic_model.get_topic(0)  # Select the most frequent topic

# topic_model.visualize_topics() #**too small dataset for distance map

topic_model.visualize_barchart()

!pip install gensim

from gensim.models import CoherenceModel
from gensim import corpora

# Tokenize
tokenized_text = [doc.split() for doc in docs]

# Extract topic words from BERTopic model
topic_words = [ [word for word, _ in topic_model.get_topic(i)] for i in range(len(set(topics)) - 1) ]

# Dictionary and corpus for gensim
id2word = corpora.Dictionary(tokenized_text)
corpus = [id2word.doc2bow(text) for text in tokenized_text]

# Coherence model
coherence_model = CoherenceModel(
    topics=topic_words,
    texts=tokenized_text,
    dictionary=id2word,
    coherence='c_v'
)

coherence_score = coherence_model.get_coherence()
print(f"BERTopic c_v coherence score: {coherence_score:.4f}")